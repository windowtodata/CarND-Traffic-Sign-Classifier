{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Deep Auto-Encoder implementation\n",
    "\t\n",
    "\tAn auto-encoder works as follows:\n",
    "\n",
    "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
    "\tsoftmax(W*x + b)  = x'\n",
    "\t\n",
    "\twhere W is matrix from R^k --> R^j\n",
    "\n",
    "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
    "\n",
    "\tso our reconstruction function is softmax'(W' * x' + b') \n",
    "\n",
    "\tNow the point of \n",
    "the auto-encoder is to create a reduction matrix (values for W, b) \n",
    "\tthat is \"good\" at reconstructing  the original data. \n",
    "\n",
    "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
    "\n",
    "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def create(x, layer_sizes):\n",
    "\n",
    "\t# Build the encoding layers\n",
    "\tnext_layer_input = x\n",
    "\n",
    "\tencoding_matrices = []\n",
    "\tfor dim in layer_sizes:\n",
    "\t\tinput_dim = int(next_layer_input.get_shape()[1])\n",
    "\n",
    "\t\t# Initialize W using random values in interval [-1/sqrt(n) , 1/sqrt(n)]\n",
    "\t\tW = tf.Variable(tf.random_uniform([input_dim, dim], -1.0 / math.sqrt(input_dim), 1.0 / math.sqrt(input_dim)))\n",
    "\n",
    "\t\t# Initialize b to zero\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\n",
    "\t\t# We are going to use tied-weights so store the W matrix for later reference.\n",
    "\t\tencoding_matrices.append(W)\n",
    "\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\n",
    "\t\t# the input into the next layer is the output of this layer\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# The fully encoded x value is now stored in the next_layer_input\n",
    "\tencoded_x = next_layer_input\n",
    "\n",
    "\t# build the reconstruction layers by reversing the reductions\n",
    "\tlayer_sizes.reverse()\n",
    "\tencoding_matrices.reverse()\n",
    "\n",
    "\n",
    "\tfor i, dim in enumerate(layer_sizes[1:] + [ int(x.get_shape()[1])]) :\n",
    "\t\t# we are using tied weights, so just lookup the encoding matrix for this step and transpose it\n",
    "\t\tW = tf.transpose(encoding_matrices[i])\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# the fully encoded and reconstructed value of x is here:\n",
    "\treconstructed_x = next_layer_input\n",
    "\n",
    "\treturn {\n",
    "\t\t'encoded': encoded_x,\n",
    "\t\t'decoded': reconstructed_x,\n",
    "\t\t'cost' : tf.sqrt(tf.reduce_mean(tf.square(x-reconstructed_x)))\n",
    "\t}\n",
    "\n",
    "def simple_test():\n",
    "\tsess = tf.Session()\n",
    "\tx = tf.placeholder(\"float\", [None, 4])\n",
    "\tautoencoder = create(x, [2])\n",
    "\tinit = tf.initialize_all_variables()\n",
    "\tsess.run(init)\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(autoencoder['cost'])\n",
    "\n",
    "\n",
    "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
    "\tc1 = np.array([0,0,0.5,0])\n",
    "\tc2 = np.array([0.5,0,0,0])\n",
    "\n",
    "\t# do 1000 training steps\n",
    "\tfor i in range(2000):\n",
    "\t\t# make a batch of 100:\n",
    "\t\tbatch = []\n",
    "\t\tfor j in range(100):\n",
    "\t\t\t# pick a random centroid\n",
    "\t\t\tif (random.random() > 0.5):\n",
    "\t\t\t\tvec = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvec = c2\n",
    "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
    "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch})\n",
    "\n",
    "\n",
    "def deep_test():\n",
    "\tsess = tf.Session()\n",
    "\tstart_dim = 5\n",
    "\tx = tf.placeholder(\"float\", [None, start_dim])\n",
    "\tautoencoder = create(x, [4, 3, 2])\n",
    "\tinit = tf.initialize_all_variables()\n",
    "\tsess.run(init)\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(autoencoder['cost'])\n",
    "\n",
    "\n",
    "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
    "\tc1 = np.zeros(start_dim)\n",
    "\tc1[0] = 1\n",
    "\n",
    "\tprint c1\n",
    "\n",
    "\tc2 = np.zeros(start_dim)\n",
    "\tc2[1] = 1\n",
    "\n",
    "\t# do 1000 training steps\n",
    "\tfor i in range(5000):\n",
    "\t\t# make a batch of 100:\n",
    "\t\tbatch = []\n",
    "\t\tfor j in range(1):\n",
    "\t\t\t# pick a random centroid\n",
    "\t\t\tif (random.random() > 0.5):\n",
    "\t\t\t\tvec = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvec = c2\n",
    "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
    "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch})\n",
    "\t\t\t#print i, \" original\", batch[0]\n",
    "\t\t\t#print i, \" decoded\", sess.run(autoencoder['decoded'], feed_dict={x: batch})\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.]\n",
      "0  cost 0.304491\n",
      "100  cost 0.0420582\n",
      "200  cost 0.144715\n",
      "300  cost 0.0622106\n",
      "400  cost 0.0892139\n",
      "500  cost 0.113777\n",
      "600  cost 0.128422\n",
      "700  cost 0.0532172\n",
      "800  cost 0.0447115\n",
      "900  cost 0.0491296\n",
      "1000  cost 0.0380971\n",
      "1100  cost 0.0906428\n",
      "1200  cost 0.019041\n",
      "1300  cost 0.0950737\n",
      "1400  cost 0.0612669\n",
      "1500  cost 0.0515554\n",
      "1600  cost 0.079839\n",
      "1700  cost 0.0732226\n",
      "1800  cost 0.0364565\n",
      "1900  cost 0.0828198\n",
      "2000  cost 0.0872677\n",
      "2100  cost 0.0543628\n",
      "2200  cost 0.103657\n",
      "2300  cost 0.0401722\n",
      "2400  cost 0.0759303\n",
      "2500  cost 0.0679583\n",
      "2600  cost 0.0996825\n",
      "2700  cost 0.00336536\n",
      "2800  cost 0.0668634\n",
      "2900  cost 0.0439719\n",
      "3000  cost 0.0414257\n",
      "3100  cost 0.0642471\n",
      "3200  cost 0.0766344\n",
      "3300  cost 0.0476606\n",
      "3400  cost 0.118215\n",
      "3500  cost 0.0687303\n",
      "3600  cost 0.0529853\n",
      "3700  cost 0.0578312\n",
      "3800  cost 0.098788\n",
      "3900  cost 0.0736846\n",
      "4000  cost 0.0731065\n",
      "4100  cost 0.0299357\n",
      "4200  cost 0.0529768\n",
      "4300  cost 0.0565388\n",
      "4400  cost 0.0798462\n",
      "4500  cost 0.0603539\n",
      "4600  cost 0.0758113\n",
      "4700  cost 0.0901957\n",
      "4800  cost 0.0634561\n",
      "4900  cost 0.0375944\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tdeep_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
